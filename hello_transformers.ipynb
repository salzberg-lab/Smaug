{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from transformers import BertConfig, BertModel, BertForMaskedLM\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_dir = '/home-3/msomme16@jhu.edu/scratch/shimmer/data/'\n",
    "CDS_path = os.path.join(data_dir, \"CDS_3600.pkl\")\n",
    "\n",
    "with open(CDS_path, 'rb') as f:\n",
    "    CDS = pickle.load(f)\n",
    "print(CDS[10])\n",
    "print(len(CDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(s) for s in CDS]\n",
    "meanlen = np.mean(lengths)\n",
    "medlen = np.median(lengths)\n",
    "minlen = np.min(lengths)\n",
    "maxlen = np.max(lengths)\n",
    "print(meanlen, medlen, minlen, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom parameters for BERT model\n",
    "vocab_size = 11 # Vocabulary size of inputs_ids in BertModel. default=30522\n",
    "hidden_size = 768 # Size of the encoder layers and the pooler layer, default=768\n",
    "num_hidden_layers = 12 # Number of hidden layers in the Transformer encoder. default=12\n",
    "num_attention_heads = 12 # Number of attention heads for each attention layer in the Transformer encoder, default=12\n",
    "intermediate_size = 3072 # The size of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder. default=3072\n",
    "hidden_act = \"gelu\" # The non-linear activation function (function or string) in the encoder and pooler. If string, “gelu”, “relu”, “swish” and “gelu_new” are supported. default=\"gelu\"\n",
    "hidden_dropout_prob = 0.1 # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler. default=0.1\n",
    "attention_probs_dropout_prob = 0.1 # The dropout ratio for the attention probabilities. default=0.1\n",
    "max_position_embeddings = 512 # The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). default=512\n",
    "type_vocab_size = 2 # 1 # The vocabulary size of the token_type_ids passed into BertModel. default=2\n",
    "initializer_range = 0.02 # The sttdev of the truncated_normal_initializer for initializing all weight matrices. default=0.02\n",
    "layer_norm_eps = 1e-12 # The epsilon used by LayerNorm. default=1e-12\n",
    "\n",
    "\n",
    "config = BertConfig(vocab_size_or_config_json_file=vocab_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_hidden_layers=num_hidden_layers,\n",
    "                    num_attention_heads=num_attention_heads,\n",
    "                    intermediate_size=intermediate_size,\n",
    "                    hidden_act=hidden_act,\n",
    "                    hidden_dropout_prob=hidden_dropout_prob,\n",
    "                    attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
    "                    max_position_embeddings=max_position_embeddings,\n",
    "                    type_vocab_size=type_vocab_size,\n",
    "                    initializer_range=initializer_range,\n",
    "                    layer_norm_eps=layer_norm_eps)\n",
    "\n",
    "# model = BertModel(config)\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "print(model)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data as GPU tensors\n",
    "max_aa_seq_length = max_position_embeddings\n",
    "\n",
    "\n",
    "def tokenize_aa_seq_murphy10(aa_seq):\n",
    "    table = {\"L\":1,\n",
    "             \"V\":1,\n",
    "             \"I\":1,\n",
    "             \"M\":1,\n",
    "             \"C\":2,\n",
    "             \"A\":3,\n",
    "             \"G\":4,\n",
    "             \"S\":5,\n",
    "             \"T\":5,\n",
    "             \"P\":6,\n",
    "             \"F\":7,\n",
    "             \"Y\":7,\n",
    "             \"W\":7,\n",
    "             \"E\":8,\n",
    "             \"D\":8,\n",
    "             \"N\":8,\n",
    "             \"Q\":8,\n",
    "             \"K\":9,\n",
    "             \"R\":9,\n",
    "             \"H\":10,\n",
    "             \"X\":0,\n",
    "             \"B\":0}\n",
    "    tokenized = [table[aa] for aa in aa_seq]\n",
    "    return tokenized\n",
    "\n",
    "tokens = [tokenize_aa_seq_murphy10(seq) for seq in CDS[:100]]\n",
    "tokens_tensor = torch.zeros(len(tokens), max_aa_seq_length, dtype=torch.long)\n",
    "for i in range(len(tokens)):\n",
    "    l = len(tokens[i]) # scuff way to ensure fit in tensor, TODO build correctly sized data set and split into train test\n",
    "    if l > max_aa_seq_length:\n",
    "        l = max_aa_seq_length\n",
    "    for j in range(l):\n",
    "        tokens_tensor[i][j] += tokens[i][j]\n",
    "\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "\n",
    "print(tokens_tensor)\n",
    "print(tokens_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    acc_list = []\n",
    "    for d in data:\n",
    "        outputs = model(d.unsqueeze(0), masked_lm_labels=d.unsqueeze(0))\n",
    "        loss, prediction_scores = outputs[:2]\n",
    "        \n",
    "        predicted_index = torch.argmax(prediction_scores, dim=2)\n",
    "        n_correct = torch.sum(predicted_index==d).item()\n",
    "#         n_possible = torch.sum(d!=0).item()\n",
    "        n_possible = len(predicted_index[0])\n",
    "        acc = n_correct/n_possible\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "    return(np.mean(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = tokens_tensor[:int(0.8*len(tokens_tensor))]\n",
    "dvalid = tokens_tensor[int(0.8*len(tokens_tensor)):]\n",
    "print(len(dtrain))\n",
    "print(len(dvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "batch_size = 1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)#, momentum=args.momentum)\n",
    "# optimizer = optim.AdamW(model.parameters())\n",
    "_max_select = len(dtrain)\n",
    "\n",
    "\n",
    "# single step to non-zero weights\n",
    "optimizer.zero_grad()\n",
    "select_idx = np.random.randint(0, _max_select, batch_size)\n",
    "outputs = model(dtrain[select_idx], masked_lm_labels=dtrain[select_idx])\n",
    "loss, prediction_scores = outputs[:2]\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# # evaluate before training\n",
    "# acc_train = evaluate(model, dtrain)\n",
    "# acc_valid = evaluate(model, dvalid)\n",
    "# print(acc_train)\n",
    "# print(acc_valid)\n",
    "\n",
    "\n",
    "# train\n",
    "model.train()\n",
    "np.random.seed(2019)\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    select_idx = np.random.randint(0, _max_select, batch_size)\n",
    "    input_ids = tokens_tensor[select_idx]\n",
    "    outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "    loss, prediction_scores = outputs[:2]\n",
    "    print(i, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     loss.backward(retain_graph=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training\n",
    "acc_train = evaluate(model, dtrain)\n",
    "acc_valid = evaluate(model, dvalid)\n",
    "print(acc_train)\n",
    "print(acc_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(dvalid[0].unsqueeze(0), masked_lm_labels=dvalid[0].unsqueeze(0))\n",
    "loss, prediction_scores = outputs[:2]\n",
    "\n",
    "predicted_index = torch.argmax(prediction_scores, dim=2)\n",
    "print(predicted_index)\n",
    "print(dvalid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "model.train()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)#, momentum=args.momentum)\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "#     select_idx = 1\n",
    "#     input_ids = tokens_tensor[select_idx].unsqueeze(0)\n",
    "#     input_ids = tokens_tensor[0:3]\n",
    "    input_ids = dtrain[0].unsqueeze(0)\n",
    "\n",
    "    outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "    loss, prediction_scores = outputs[:2]\n",
    "    print(i, loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#     loss.backward(retain_graph=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "select_idx = 5\n",
    "input_ids = tokens_tensor[select_idx].unsqueeze(0)\n",
    "outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "loss, prediction_scores = outputs[:2]\n",
    "\n",
    "predicted_index = torch.argmax(prediction_scores, dim=2)\n",
    "a = evaluate(model, tokens_tensor[select_idx].unsqueeze(0))\n",
    "print(a)\n",
    "# print(predicted_index.shape)\n",
    "print(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on random data to ensure accuracy metric works\n",
    "model.eval()\n",
    "\n",
    "np.random.seed(2019)\n",
    "noise = np.random.randint(1, 10, 512).reshape(1,-1)\n",
    "input_ids = torch.from_numpy(noise).to('cuda')\n",
    "\n",
    "outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "loss, prediction_scores = outputs[:2]\n",
    "\n",
    "predicted_index = torch.argmax(prediction_scores, dim=2)\n",
    "a = evaluate(model, tokens_tensor[select_idx].unsqueeze(0))\n",
    "print(a)\n",
    "# print(predicted_index.shape)\n",
    "print(input_ids)\n",
    "print(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix forward mask\n",
    "model.eval()\n",
    "\n",
    "np.random.seed(2019)\n",
    "noise = np.random.randint(1, 10, 512).reshape(1,-1)\n",
    "input_ids = torch.from_numpy(noise).to('cuda')\n",
    "input_ids[0,0] = -inf\n",
    "\n",
    "attention_mask = torch.from_numpy(np.ones(512).reshape(1,-1)).float().to('cuda')\n",
    "attention_mask[0,0] = 0\n",
    "# print(attention_mask)\n",
    "\n",
    "token_type_ids = torch.from_numpy(np.zeros(512).reshape(1,-1)).long().to('cuda')\n",
    "\n",
    "outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "loss, prediction_scores = outputs[:2]\n",
    "\n",
    "predicted_index = torch.argmax(prediction_scores, dim=2)\n",
    "a = evaluate(model, tokens_tensor[select_idx].unsqueeze(0))\n",
    "print(a)\n",
    "# print(predicted_index.shape)\n",
    "print(input_ids)\n",
    "print(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_epoch(epoch, args, model, device, data_loader, optimizer):\n",
    "    model.train()  # set to training mode, disappointingly does not actually train the model \n",
    "    pid = os.getpid()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "            \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)#, momentum=args.momentum)\n",
    "optimizer.zero_grad()\n",
    "# output = model(tokens_tensor)\n",
    "# loss = F.nll_loss(output, target.to('cuda'))\n",
    "print([x for x in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "loss, prediction_scores = outputs[:2]\n",
    "print(loss)\n",
    "print(prediction_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_ids.shape)\n",
    "print(tokens_tensor[0].unsqueeze(0).shape)\n",
    "print(tokens_tensor[0:2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM(config)\n",
    "model.to('cuda')\n",
    "\n",
    "# input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "# input_ids = tokens_tensor[0].unsqueeze(0)\n",
    "input_ids = tokens_tensor[0:2]\n",
    "\n",
    "outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "loss, prediction_scores = outputs[:2]\n",
    "print(loss)\n",
    "print(prediction_scores.shape)\n",
    "\n",
    "for i in range(2):\n",
    "    input_ids = tokens_tensor[0:2]\n",
    "    outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "    loss, prediction_scores = outputs[:2]\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    print(prediction_scores.shape)\n",
    "#     loss.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2040, 2001, 3958, 27227, 1029, 102, 3958, 27227, 2001, 1037, 13997, 11510, 102]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logging.basicConfig(level=logging.NONE)\n",
    "\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "# masked_index = 8\n",
    "# tokenized_text[masked_index] = '[MASK]'\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "print(indexed_tokens)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "henson\n",
      "puppet\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "# segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "#     outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    outputs = model(tokens_tensor)\n",
    "\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)\n",
    "\n",
    "predicted_index = torch.argmax(predictions[0, 11]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      "was\n",
      "jim\n",
      "henson\n",
      "?\n",
      ".\n",
      "jim\n",
      "henson\n",
      "was\n",
      "a\n",
      "puppet\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(indexed_tokens)):\n",
    "    predicted_index = torch.argmax(predictions[0, i]).item()\n",
    "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "    print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    }
   ],
   "source": [
    "predicted_token = tokenizer.convert_ids_to_tokens([103])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example tokenization\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "print(tokenized_text)\n",
    "\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "print(tokens_tensor)\n",
    "print(tokens_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "# segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "#     outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "print(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "\n",
    "\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_text = \"Martin Steinegger is in Peru because he \"\n",
    "# original_text = \"The real reason Steven keeps recruiting German postdocs is \"\n",
    "# original_text = \"The secret to giving a fun and compelling Joint Lab Meeting presentation is \"\n",
    "# original_text = \"Johns Hopkins University is\"\n",
    "# original_text = \"UC Berkeley is\"\n",
    "# original_text = \"Why did Donald Trump \"\n",
    "# original_text = \"Finding genes is easy... The secret is \"\n",
    "# original_text = \"The reason I want to get a PhD in Biomedical Engineering is \"\n",
    "# original_text = \"Computational gene finding is easy, the real secret is \"\n",
    "# original_text = \"Computational gene finding is easy, the problem is \"\n",
    "# original_text = \"I came to Johns Hopkins University because \"\n",
    "# original_text = \"My experiment does not work because \"\n",
    "# original_text = \"Ariel Isser is a great leader for BME EDGE, the only thing he could do to improve is \"\n",
    "# original_text = \"Silicon fusion bonding is  \"\n",
    "original_text = \"The future of Biomedical Engineering is \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = original_text\n",
    "for i in range(20): # not the best way to iterate, but it works\n",
    "    if text[-1] != \".\":\n",
    "        # Encode a text inputs\n",
    "        indexed_tokens = tokenizer.encode(text)\n",
    "\n",
    "        # Convert indexed tokens in a PyTorch tensor\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "        # If you have a GPU, put everything on cuda\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "\n",
    "        # Predict all tokens\n",
    "        with torch.no_grad():\n",
    "            outputs = model(tokens_tensor)\n",
    "            predictions = outputs[0]\n",
    "\n",
    "        # get the predicted next sub-word (in our case, the word 'man')\n",
    "        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "        predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
    "\n",
    "        text = predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original text:\\t\\t\", original_text)\n",
    "print(\"Completed sentence:\\t\", predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:transformers-pytorch]",
   "language": "python",
   "name": "conda-env-transformers-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
