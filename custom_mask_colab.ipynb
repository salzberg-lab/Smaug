{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"custom_mask_colab.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"03irV-C_6jpZ","colab_type":"code","outputId":"a2868530-ea56-4cd5-cf6b-bc351d9cc042","executionInfo":{"status":"ok","timestamp":1571679724724,"user_tz":240,"elapsed":9553,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n","\u001b[K     |████████████████████████████████| 317kB 1.4MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.16.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Collecting regex (from transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n","\u001b[K     |████████████████████████████████| 645kB 41.9MB/s \n","\u001b[?25hCollecting sacremoses (from transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 37.8MB/s \n","\u001b[?25hCollecting sentencepiece (from transformers)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 43.9MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.9.251)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n","Requirement already satisfied: botocore<1.13.0,>=1.12.251 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.12.251)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.251->boto3->transformers) (2.5.3)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.251->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=0a17ca3ad8ff54d6bd42f61b788689e4ebadecaf863346cb1b3515456e907c2f\n","  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n","Successfully built sacremoses\n","Installing collected packages: regex, sacremoses, sentencepiece, transformers\n","Successfully installed regex-2019.8.19 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X3i73FlB55Xt","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","import torch\n","import numpy as np\n","import torch.optim as optim\n","from transformers import BertConfig, BertModel, BertForMaskedLM\n","\n","# import matplotlib.pyplot as plt\n","# from transformers import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MTLDde1m6k_O","colab_type":"code","outputId":"9b495eb8-929f-4a26-f2e9-ed09e91c0fac","executionInfo":{"status":"ok","timestamp":1571679766149,"user_tz":240,"elapsed":20823,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kBbEF2Uc6lK5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4db79780-660f-4d5a-cfd3-2646546eae39","executionInfo":{"status":"ok","timestamp":1571679869536,"user_tz":240,"elapsed":732,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}}},"source":["d = \"drive/My Drive/Colab Notebooks/smaug/data\"\n","p = os.path.join(d, \"CDS_singlestrain_threeprime.pkl\")\n","print(os.listdir(d))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["['CDS_singlestrain_threeprime.pkl']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cA49xxRjaakk","colab_type":"code","colab":{}},"source":["with open(p, 'rb') as f:\n","  CDS = pickle.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXjv4EukaaiC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"36a19261-0cf3-42e6-c8d7-c1f9190a8139","executionInfo":{"status":"ok","timestamp":1571680019193,"user_tz":240,"elapsed":499,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}}},"source":["print(type(CDS))\n","print(len(CDS))\n","print(CDS[100])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<class 'list'>\n","9304394\n","HVSLLSVHAAVAVWRKKRQMYIDQYCVRGTKLTNAEKFVFTMYATVVGIKEDLMRVDASDINVSLIEQRRLNRIVDRRTMKNGDPSILIFDNAFIRELTVSDKSPYSRIWDENMRKLTEVSKNQAHLQCTFVMVLSFLLIAKM\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KVf-Zgp255X6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"af40d7b3-bddf-4197-e983-cc739b421bf6","executionInfo":{"status":"ok","timestamp":1571680333336,"user_tz":240,"elapsed":6481,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}}},"source":["lengths = [len(s) for s in CDS]\n","meanlen = np.mean(lengths)\n","medlen = np.median(lengths)\n","minlen = np.min(lengths)\n","maxlen = np.max(lengths)\n","print(meanlen, medlen, minlen, maxlen)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["345.9973719943502 302.0 11 39686\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EmIQPDvZ55X9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YMj7-AzZ55YA","colab_type":"code","colab":{}},"source":["# import logging\n","# logging.basicConfig(level=logging.INFO)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dvEYlPOb55YD","colab_type":"code","colab":{}},"source":["# # custom parameters for BERT model\n","# vocab_size = 12 # Vocabulary size of inputs_ids in BertModel. default=30522\n","# hidden_size = 768 # Size of the encoder layers and the pooler layer, default=768\n","# num_hidden_layers = 12 # Number of hidden layers in the Transformer encoder. default=12\n","# num_attention_heads = 12 # Number of attention heads for each attention layer in the Transformer encoder, default=12\n","# intermediate_size = 3072 # The size of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder. default=3072\n","# hidden_act = \"gelu\" # The non-linear activation function (function or string) in the encoder and pooler. If string, “gelu”, “relu”, “swish” and “gelu_new” are supported. default=\"gelu\"\n","# hidden_dropout_prob = 0.1 # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler. default=0.1\n","# attention_probs_dropout_prob = 0.1 # The dropout ratio for the attention probabilities. default=0.1\n","# max_position_embeddings = 512 # The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). default=512\n","# type_vocab_size = 2 # 1 # The vocabulary size of the token_type_ids passed into BertModel. default=2\n","# initializer_range = 0.02 # The sttdev of the truncated_normal_initializer for initializing all weight matrices. default=0.02\n","# layer_norm_eps = 1e-12 # The epsilon used by LayerNorm. default=1e-12\n","\n","\n","# config = BertConfig(vocab_size_or_config_json_file=vocab_size,\n","#                     hidden_size=hidden_size,\n","#                     num_hidden_layers=num_hidden_layers,\n","#                     num_attention_heads=num_attention_heads,\n","#                     intermediate_size=intermediate_size,\n","#                     hidden_act=hidden_act,\n","#                     hidden_dropout_prob=hidden_dropout_prob,\n","#                     attention_probs_dropout_prob=attention_probs_dropout_prob,\n","#                     max_position_embeddings=max_position_embeddings,\n","#                     type_vocab_size=type_vocab_size,\n","#                     initializer_range=initializer_range,\n","#                     layer_norm_eps=layer_norm_eps)\n","\n","# model = BertForMaskedLM(config)\n","\n","# print(model)\n","# model.to('cuda')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8Ra4nFr55YF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"bae34958-842d-4b05-a183-b5663aca573b","executionInfo":{"status":"ok","timestamp":1571680968687,"user_tz":240,"elapsed":2933,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}}},"source":["# custom parameters for BERT model\n","vocab_size = 12 # Vocabulary size of inputs_ids in BertModel. default=30522\n","hidden_size = 768 # Size of the encoder layers and the pooler layer, default=768\n","num_hidden_layers = 12 # Number of hidden layers in the Transformer encoder. default=12\n","num_attention_heads = 12 # Number of attention heads for each attention layer in the Transformer encoder, default=12\n","intermediate_size = hidden_size*4 # The size of the “intermediate” (i.e., feed-forward) layer in the Transformer encoder. default=3072\n","hidden_act = \"gelu\" # The non-linear activation function (function or string) in the encoder and pooler. If string, “gelu”, “relu”, “swish” and “gelu_new” are supported. default=\"gelu\"\n","hidden_dropout_prob = 0.1 # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler. default=0.1\n","attention_probs_dropout_prob = 0.1 # The dropout ratio for the attention probabilities. default=0.1\n","max_position_embeddings = 10 # The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). default=512\n","type_vocab_size = 1 # 1 # The vocabulary size of the token_type_ids passed into BertModel. default=2\n","initializer_range = 0.02 # The sttdev of the truncated_normal_initializer for initializing all weight matrices. default=0.02\n","layer_norm_eps = 1e-12 # The epsilon used by LayerNorm. default=1e-12\n","\n","\n","config = BertConfig(vocab_size_or_config_json_file=vocab_size,\n","                    hidden_size=hidden_size,\n","                    num_hidden_layers=num_hidden_layers,\n","                    num_attention_heads=num_attention_heads,\n","                    intermediate_size=intermediate_size,\n","                    hidden_act=hidden_act,\n","                    hidden_dropout_prob=hidden_dropout_prob,\n","                    attention_probs_dropout_prob=attention_probs_dropout_prob,\n","                    max_position_embeddings=max_position_embeddings,\n","                    type_vocab_size=type_vocab_size,\n","                    initializer_range=initializer_range,\n","                    layer_norm_eps=layer_norm_eps)\n","\n","model = BertForMaskedLM(config)\n","\n","print(model)\n","model.to('cuda')"],"execution_count":36,"outputs":[{"output_type":"stream","text":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(12, 768, padding_idx=0)\n","      (position_embeddings): Embedding(10, 768)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=12, bias=False)\n","    )\n","  )\n",")\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(12, 768, padding_idx=0)\n","      (position_embeddings): Embedding(10, 768)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=12, bias=False)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"rf0FLgnQ55YI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"02756d0c-b5e7-4c62-bc29-e2a4560c6053","executionInfo":{"status":"ok","timestamp":1571680972036,"user_tz":240,"elapsed":2423,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}}},"source":["# encode data as GPU tensors\n","max_aa_seq_length = max_position_embeddings\n","\n","\n","def tokenize_aa_seq_murphy10(aa_seq):\n","    table = {\"L\":1,\n","             \"V\":1,\n","             \"I\":1,\n","             \"M\":1,\n","             \"C\":2,\n","             \"A\":3,\n","             \"G\":4,\n","             \"S\":5,\n","             \"T\":5,\n","             \"P\":6,\n","             \"F\":7,\n","             \"Y\":7,\n","             \"W\":7,\n","             \"E\":8,\n","             \"D\":8,\n","             \"N\":8,\n","             \"Q\":8,\n","             \"K\":9,\n","             \"R\":9,\n","             \"H\":10,\n","             \"X\":0, # get rid of those\n","             \"B\":0,\n","             \"*\":0}\n","    tokenized = [table[aa] for aa in aa_seq]\n","    return tokenized\n","\n","tokens = [tokenize_aa_seq_murphy10(seq) for seq in CDS[:10000]] ######################################\n","tokens_tensor = torch.zeros(len(tokens), max_aa_seq_length, dtype=torch.long)\n","for i in range(len(tokens)):\n","    l = len(tokens[i]) # scuff way to ensure fit in tensor, TODO build correctly sized data set and split into train test\n","    if l > max_aa_seq_length:\n","        l = max_aa_seq_length\n","    for j in range(l):\n","        tokens_tensor[i][j] += tokens[i][j]\n","\n","tokens_tensor = tokens_tensor.to('cuda')\n","\n","print(tokens_tensor)\n","print(tokens_tensor.shape)"],"execution_count":37,"outputs":[{"output_type":"stream","text":["tensor([[ 9,  1,  1,  ...,  6,  9,  5],\n","        [ 3,  8,  3,  ...,  8,  4,  9],\n","        [ 8,  9,  1,  ...,  1,  8,  1],\n","        ...,\n","        [ 7,  5,  9,  ...,  9,  4,  8],\n","        [ 4, 10,  9,  ...,  5,  7,  5],\n","        [ 8,  3,  7,  ...,  3,  1,  7]], device='cuda:0')\n","torch.Size([10000, 10])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wkyp8xJa55YK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zlHdsHkb55YM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"48341dd0-0de3-4298-91e2-9248a9bf05e3","executionInfo":{"status":"ok","timestamp":1571680974280,"user_tz":240,"elapsed":592,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}}},"source":["dtrain = tokens_tensor[:int(0.8*len(tokens_tensor))] #TODO do an actual random selection on better data\n","dvalid = tokens_tensor[int(0.8*len(tokens_tensor)):]\n","print(len(dtrain))\n","print(len(dvalid))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["8000\n","2000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vzh6lkJpcfTW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KUuZzFiD55YQ","colab_type":"code","colab":{}},"source":["optimizer = optim.SGD(model.parameters(), lr=0.0001)#, momentum=args.momentum)\n","# optimizer = optim.AdamW(model.parameters())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7HkebPG55YS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d2c6e2a3-d7aa-4403-a118-bc97f738bad9","executionInfo":{"status":"error","timestamp":1571681057623,"user_tz":240,"elapsed":80063,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}}},"source":["# train model with single aa masked at a time\n","model.train()\n","loss_mask = (torch.zeros(max_position_embeddings)-1).long().to('cuda')\n","\n","np.random.seed(2019)\n","for i in range(1000):\n","    select_idx = np.random.randint(0,len(dtrain),1)[0]    \n","    input_ids = dtrain[select_idx].unsqueeze(0)\n","\n","    for j in range(max_position_embeddings):\n","        tempval = 0\n","        tempval += input_ids[0, j].item()\n","        if tempval != 0:\n","            optimizer.zero_grad()\n","\n","            loss_mask[j] = tempval # calculate loss based only on masked amino acid\n","            input_ids[0, j] = vocab_size-1 # mask label is the highest vocab number, never present in data\n","\n","            outputs = model(input_ids, masked_lm_labels=loss_mask) #TODO mask padding 0's for attention\n","            loss, prediction_scores = outputs[:2]\n","            print(i, torch.argmax(prediction_scores[0,j]).item(), loss.item())\n","            loss.backward()\n","            optimizer.step()\n","\n","            input_ids[0,j] = tempval # set masked value back to original value\n","            loss_mask[j] = -1\n","\n","#             if j==10:\n","#                 break\n","        else:\n","            break # stop training at end of protein sequence\n","\n","#     break"],"execution_count":40,"outputs":[{"output_type":"stream","text":["0 7 2.7171647548675537\n","0 10 3.0843400955200195\n","0 7 2.4902782440185547\n","0 1 2.320706605911255\n","0 11 3.0093748569488525\n","0 0 2.0599899291992188\n","0 11 2.66562557220459\n","0 3 2.693415880203247\n","0 3 2.078138828277588\n","0 3 2.7851948738098145\n","1 3 2.450014114379883\n","1 11 2.883767604827881\n","1 9 1.8086891174316406\n","1 3 2.520387887954712\n","1 9 2.4201865196228027\n","1 3 2.0110766887664795\n","1 9 2.013474941253662\n","1 9 2.6840691566467285\n","1 3 2.477153778076172\n","1 3 2.505335569381714\n","2 9 2.304896593093872\n","2 9 3.0024807453155518\n","2 9 3.5260696411132812\n","2 9 1.7779908180236816\n","2 8 2.7323832511901855\n","2 9 1.5526410341262817\n","2 5 2.154095411300659\n","2 9 2.0327353477478027\n","2 9 2.009233236312866\n","2 1 1.684960126876831\n","3 1 1.6441752910614014\n","3 1 2.189732313156128\n","3 9 3.103215217590332\n","3 1 2.9916276931762695\n","3 1 3.4715466499328613\n","3 9 2.697211742401123\n","3 1 1.880605936050415\n","3 8 2.073195219039917\n","3 1 2.3447329998016357\n","3 1 2.7612595558166504\n","4 8 1.7580616474151611\n","4 8 1.4601887464523315\n","4 9 2.150949001312256\n","4 8 1.5622831583023071\n","4 1 1.9537913799285889\n","4 8 3.341904640197754\n","4 8 3.723763942718506\n","4 1 3.437727689743042\n","4 1 3.420773506164551\n","4 1 1.9993906021118164\n","5 8 1.2076308727264404\n","5 8 2.7899880409240723\n","5 8 1.7663623094558716\n","5 8 1.6684585809707642\n","5 8 1.7004891633987427\n","5 9 2.4665303230285645\n","5 9 2.0117504596710205\n","5 9 2.0946357250213623\n","5 8 2.7497763633728027\n","5 9 2.817230224609375\n","6 1 1.7357546091079712\n","6 9 1.2329269647598267\n","6 9 1.0832921266555786\n","6 9 1.1593670845031738\n","6 9 1.8159441947937012\n","6 9 2.1201233863830566\n","6 9 1.6272364854812622\n","6 9 3.077411413192749\n","6 9 1.3304882049560547\n","6 9 3.345721960067749\n","7 9 2.357194662094116\n","7 9 1.4445596933364868\n","7 9 2.3675107955932617\n","7 9 2.388733386993408\n","7 3 1.578632116317749\n","7 9 0.8101744651794434\n","7 9 3.3433351516723633\n","7 9 1.3771787881851196\n","7 9 2.673701286315918\n","7 9 3.0742766857147217\n","8 9 2.0853896141052246\n","8 9 2.983243942260742\n","8 9 3.2212281227111816\n","8 9 1.4221433401107788\n","8 9 3.0332064628601074\n","8 9 2.511422872543335\n","8 8 2.4406471252441406\n","8 6 1.8521840572357178\n","8 8 2.8356363773345947\n","8 1 2.8789427280426025\n","9 9 1.7001011371612549\n","9 9 2.2211575508117676\n","9 1 2.310197353363037\n","9 9 2.591485023498535\n","9 8 1.5333516597747803\n","9 3 2.456387758255005\n","9 8 3.41825795173645\n","9 8 1.6421010494232178\n","9 8 1.6409708261489868\n","9 8 1.3740084171295166\n","10 8 2.624242067337036\n","10 8 2.112624406814575\n","10 9 1.9244623184204102\n","10 8 2.7374022006988525\n","10 8 2.3015432357788086\n","10 8 1.8415660858154297\n","10 8 3.8369927406311035\n","10 8 3.8905038833618164\n","10 1 1.7250370979309082\n","10 1 1.8029451370239258\n","11 1 2.3301796913146973\n","11 8 1.6446449756622314\n","11 8 2.256270408630371\n","11 8 1.3790825605392456\n","11 8 2.986776828765869\n","11 8 1.5247588157653809\n","11 8 1.8241102695465088\n","11 8 3.1157424449920654\n","11 8 0.8552799224853516\n","11 8 2.6757960319519043\n","12 8 2.989016056060791\n","12 8 2.7533397674560547\n","12 8 2.4491968154907227\n","12 8 1.329864740371704\n","12 8 1.8380053043365479\n","12 8 2.983797311782837\n","12 8 1.214559555053711\n","12 8 1.6654607057571411\n","12 1 2.8038015365600586\n","12 8 1.496628999710083\n","13 8 2.22194766998291\n","13 8 1.421318769454956\n","13 8 1.7440203428268433\n","13 8 2.923177719116211\n","13 8 2.47278094291687\n","13 3 1.7048420906066895\n","13 8 2.348789691925049\n","13 8 2.915370225906372\n","13 8 2.877370834350586\n","13 8 1.4632084369659424\n","14 8 1.9091535806655884\n","14 8 1.6573675870895386\n","14 8 3.5147974491119385\n","14 8 2.7289819717407227\n","14 9 3.5646231174468994\n","14 8 2.279087543487549\n","14 8 2.3664608001708984\n","14 9 1.6989907026290894\n","14 1 2.6393377780914307\n","14 1 3.158602237701416\n","15 1 1.7622638940811157\n","15 8 1.839159369468689\n","15 1 1.9776341915130615\n","15 1 2.0610485076904297\n","15 9 2.4397575855255127\n","15 1 1.6978245973587036\n","15 8 1.3549917936325073\n","15 8 1.3696656227111816\n","15 8 0.8395383358001709\n","15 8 2.3324191570281982\n","16 8 3.2867658138275146\n","16 8 2.368408441543579\n","16 8 2.0980517864227295\n","16 8 1.0528596639633179\n","16 8 1.447366714477539\n","16 8 2.1372666358947754\n","16 8 1.7370235919952393\n","16 1 2.95882511138916\n","16 8 2.746274471282959\n","16 1 1.2944186925888062\n","17 8 2.02811336517334\n","17 8 3.009171962738037\n","17 8 2.9449832439422607\n","17 1 1.4972621202468872\n","17 8 1.7950775623321533\n","17 1 1.4852294921875\n","17 1 2.4551522731781006\n","17 1 1.6462500095367432\n","17 8 1.0156961679458618\n","17 8 3.4673244953155518\n","18 8 1.666914463043213\n","18 1 2.4709250926971436\n","18 8 1.6357477903366089\n","18 8 3.000624656677246\n","18 1 1.1754902601242065\n","18 1 0.981197714805603\n","18 1 0.8619515895843506\n","18 1 3.2097349166870117\n","18 1 2.702186346054077\n","18 1 2.586480140686035\n","19 1 2.8650693893432617\n","19 1 2.024592876434326\n","19 1 1.6158851385116577\n","19 1 2.834805488586426\n","19 3 2.65333890914917\n","19 3 2.5151710510253906\n","19 3 1.909557819366455\n","19 8 3.1772830486297607\n","19 3 1.7496249675750732\n","19 3 2.5634913444519043\n","20 8 1.7210479974746704\n","20 1 2.06929349899292\n","20 1 2.3270812034606934\n","20 8 1.9618432521820068\n","20 9 1.5789679288864136\n","20 9 2.5829074382781982\n","20 1 2.1997251510620117\n","20 8 1.382637619972229\n","20 8 1.1770446300506592\n","20 8 2.152069091796875\n","21 8 4.559577941894531\n","21 8 2.327885150909424\n","21 8 2.2806079387664795\n","21 8 1.5652412176132202\n","21 8 1.3989275693893433\n","21 8 3.3490443229675293\n","21 8 1.0938278436660767\n","21 8 2.033102512359619\n","21 8 1.8039438724517822\n","21 8 1.6600056886672974\n","22 3 1.3183397054672241\n","22 3 2.1770315170288086\n","22 3 1.5969359874725342\n","22 8 1.9377634525299072\n","22 1 2.9339587688446045\n","22 3 1.9764842987060547\n","22 8 4.1317458152771\n","22 3 1.5588699579238892\n","22 8 3.0531344413757324\n","22 1 1.645439863204956\n","23 8 2.7819595336914062\n","23 8 1.4986183643341064\n","23 8 1.2931244373321533\n","23 8 0.7014472484588623\n","23 8 0.6674718856811523\n","23 8 1.9234700202941895\n","23 8 0.7165157794952393\n","23 8 0.6953229904174805\n","23 8 2.284932851791382\n","23 8 2.640397548675537\n","24 8 3.4223384857177734\n","24 8 1.0306963920593262\n","24 8 2.6361427307128906\n","24 8 3.429403781890869\n","24 8 2.5987234115600586\n","24 8 2.24465274810791\n","24 8 3.212005853652954\n","24 8 1.9906061887741089\n","24 8 1.8974663019180298\n","24 1 2.7213923931121826\n","25 8 1.254693865776062\n","25 8 1.9517207145690918\n","25 8 2.414937734603882\n","25 8 1.5505260229110718\n","25 8 1.9203145503997803\n","25 8 1.477921962738037\n","25 8 2.088149309158325\n","25 8 0.928091287612915\n","25 8 0.8361310958862305\n","25 8 1.6193063259124756\n","26 1 2.4865660667419434\n","26 9 1.5837651491165161\n","26 8 1.303436040878296\n","26 8 3.1578574180603027\n","26 8 0.9148986339569092\n","26 8 0.7961199283599854\n","26 8 3.5145041942596436\n","26 8 0.7698259353637695\n","26 8 3.104459047317505\n","26 8 2.195856809616089\n","27 8 1.235133171081543\n","27 8 1.901200532913208\n","27 8 2.1354942321777344\n","27 1 2.277827262878418\n","27 8 3.418097734451294\n","27 1 2.951166868209839\n","27 1 2.916611671447754\n","27 8 2.76979398727417\n","27 8 2.135572671890259\n","27 8 2.755265712738037\n","28 1 1.326415777206421\n","28 1 2.2217838764190674\n","28 1 1.3333775997161865\n","28 1 1.3502792119979858\n","28 1 2.0272598266601562\n","28 1 1.4844478368759155\n","28 8 2.9026834964752197\n","28 8 3.248331069946289\n","28 8 2.3443140983581543\n","28 8 3.0694925785064697\n","29 1 1.947899341583252\n","29 8 2.7137022018432617\n","29 8 3.409550189971924\n","29 8 2.05122709274292\n","29 8 1.4736477136611938\n","29 1 1.8703398704528809\n","29 8 4.152065753936768\n","29 8 3.023355722427368\n","29 3 1.5047109127044678\n","29 1 1.436429738998413\n","30 1 1.8046501874923706\n","30 1 2.2169578075408936\n","30 1 1.6901240348815918\n","30 8 1.3313838243484497\n","30 8 1.1015076637268066\n","30 8 3.525226354598999\n","30 8 1.8043506145477295\n","30 8 0.9012172222137451\n","30 8 2.539174795150757\n","30 8 2.2884011268615723\n","31 8 1.197460412979126\n","31 9 1.9153156280517578\n","31 8 1.9526327848434448\n","31 8 1.5372015237808228\n","31 8 2.4860639572143555\n","31 8 1.9091063737869263\n","31 8 2.2910633087158203\n","31 8 3.3082828521728516\n","31 8 1.2154134511947632\n","31 1 2.117635488510132\n","32 8 1.8242919445037842\n","32 9 1.956801414489746\n","32 1 1.9181865453720093\n","32 1 1.5486319065093994\n","32 8 2.550299644470215\n","32 9 1.7076417207717896\n","32 8 2.094362735748291\n","32 8 2.1934001445770264\n","32 8 1.8163466453552246\n","32 9 3.0598247051239014\n","33 9 2.724818706512451\n","33 9 3.6675755977630615\n","33 9 1.8450045585632324\n","33 1 1.6320528984069824\n","33 8 1.1568048000335693\n","33 9 2.7880465984344482\n","33 8 1.7226295471191406\n","33 8 1.8004423379898071\n","33 8 3.009403944015503\n","33 8 1.4795953035354614\n","34 8 1.350797414779663\n","34 5 1.7932058572769165\n","34 8 2.170748710632324\n","34 1 2.1989574432373047\n","34 8 3.155195474624634\n","34 9 1.3040558099746704\n","34 8 1.4228965044021606\n","34 8 2.292116641998291\n","34 8 2.852722644805908\n","34 9 1.4590113162994385\n","35 8 1.1323765516281128\n","35 9 1.1400177478790283\n","35 9 1.545752763748169\n","35 9 2.0940518379211426\n","35 8 2.894239664077759\n","35 9 2.6122560501098633\n","35 8 1.1947462558746338\n","35 8 1.7449467182159424\n","35 8 2.143394708633423\n","35 1 1.5676685571670532\n","36 8 1.6653103828430176\n","36 9 1.4220421314239502\n","36 8 1.2731908559799194\n","36 8 1.8072154521942139\n","36 8 3.1450088024139404\n","36 9 3.170839786529541\n","36 1 1.2741529941558838\n","36 1 1.5245208740234375\n","36 8 2.0340566635131836\n","36 1 2.595684051513672\n","37 1 2.6203253269195557\n","37 1 1.7109551429748535\n","37 1 1.263487458229065\n","37 1 2.1736063957214355\n","37 1 2.351526975631714\n","37 1 2.1285691261291504\n","37 1 3.042598247528076\n","37 8 2.709702491760254\n","37 8 1.4682575464248657\n","37 5 2.4867405891418457\n","38 8 1.5255829095840454\n","38 8 4.476224899291992\n","38 8 1.1808984279632568\n","38 8 1.1142029762268066\n","38 8 2.1840341091156006\n","38 8 1.4356766939163208\n","38 8 3.954582929611206\n","38 8 1.1698907613754272\n","38 8 2.8864052295684814\n","38 8 4.2171502113342285\n","39 8 3.1722912788391113\n","39 9 1.3646526336669922\n","39 8 1.2519469261169434\n","39 8 3.345932960510254\n","39 8 1.5252044200897217\n","39 9 2.308225631713867\n","39 8 2.146411895751953\n","39 8 1.8328802585601807\n","39 1 3.3174095153808594\n","39 1 1.1113126277923584\n","40 1 1.3821014165878296\n","40 1 2.0323643684387207\n","40 9 1.473160982131958\n","40 1 2.7894108295440674\n","40 1 2.728724241256714\n","40 1 2.1610069274902344\n","40 1 2.7178127765655518\n","40 1 1.9714758396148682\n","40 8 1.2502098083496094\n","40 1 2.510162115097046\n","41 8 3.4508328437805176\n","41 8 1.6360896825790405\n","41 1 4.148284912109375\n","41 8 1.5211690664291382\n","41 8 1.6368099451065063\n","41 9 1.6988320350646973\n","41 1 1.6054590940475464\n","41 1 3.369450330734253\n","41 8 2.417835235595703\n","41 1 3.3531136512756348\n","42 8 1.4200955629348755\n","42 1 1.827406883239746\n","42 1 1.7903944253921509\n","42 1 1.4656295776367188\n","42 8 3.3075785636901855\n","42 9 2.591280698776245\n","42 1 2.3058953285217285\n","42 8 1.2969632148742676\n","42 8 2.4921152591705322\n","42 1 2.5449976921081543\n","43 8 1.6184619665145874\n","43 5 2.6904008388519287\n","43 5 1.608729362487793\n","43 1 2.8775861263275146\n","43 1 2.5757408142089844\n","43 1 3.5562541484832764\n","43 1 3.1496424674987793\n","43 1 2.639187812805176\n","43 8 2.172633171081543\n","43 1 1.2365046739578247\n","44 1 3.358752489089966\n","44 1 2.129880905151367\n","44 1 1.8736348152160645\n","44 1 2.2937123775482178\n","44 1 1.5880606174468994\n","44 1 2.4342403411865234\n","44 1 1.7563847303390503\n","44 1 1.9523617029190063\n","44 1 2.1501076221466064\n","44 1 3.3631842136383057\n","45 4 1.9169121980667114\n","45 5 1.947676181793213\n","45 9 2.433875560760498\n","45 1 1.9616118669509888\n","45 8 1.9274158477783203\n","45 1 3.068345308303833\n","45 8 1.9770934581756592\n","45 8 2.0665040016174316\n","45 8 2.443678379058838\n","45 1 1.7809455394744873\n","46 9 3.270195245742798\n","46 9 2.352248430252075\n","46 9 3.284228801727295\n","46 1 2.916687488555908\n","46 9 2.1893973350524902\n","46 9 2.844050645828247\n","46 9 1.4527850151062012\n","46 8 1.7250338792800903\n","46 8 1.5150446891784668\n","46 9 1.710188865661621\n","47 8 1.9535212516784668\n","47 9 0.9290039539337158\n","47 9 2.236523389816284\n","47 9 1.6961368322372437\n","47 8 1.4492746591567993\n","47 9 2.1966311931610107\n","47 1 1.3477948904037476\n","47 8 1.3654546737670898\n","47 1 2.8170182704925537\n","47 1 2.35719633102417\n","48 8 2.2336769104003906\n","48 8 1.4527051448822021\n","48 8 1.2781214714050293\n","48 1 1.4719301462173462\n","48 8 1.7787530422210693\n","48 9 2.0489859580993652\n","48 8 2.7231502532958984\n","48 8 2.042867422103882\n","48 1 3.139256000518799\n","48 8 3.870716094970703\n","49 8 1.9551950693130493\n","49 9 2.045682430267334\n","49 8 2.0511484146118164\n","49 1 1.9552600383758545\n","49 9 1.8265118598937988\n","49 9 2.817859649658203\n","49 8 1.2343928813934326\n","49 8 1.732061505317688\n","49 8 1.741696834564209\n","49 1 1.8578948974609375\n","50 1 1.9251686334609985\n","50 9 3.089052200317383\n","50 1 1.4995747804641724\n","50 1 2.3056368827819824\n","50 1 1.7422099113464355\n","50 9 1.4764235019683838\n","50 1 2.9720332622528076\n","50 9 1.4765667915344238\n","50 9 1.8690433502197266\n","50 9 2.893939733505249\n","51 9 2.6144542694091797\n","51 9 2.3112661838531494\n","51 1 1.351529836654663\n","51 1 2.369680881500244\n","51 1 2.026002883911133\n","51 9 1.9750702381134033\n","51 1 1.1426172256469727\n","51 1 1.6450120210647583\n","51 1 1.5503299236297607\n","51 1 3.285512924194336\n","52 5 1.7947596311569214\n","52 9 1.6103184223175049\n","52 8 1.815486192703247\n","52 1 1.9535534381866455\n","52 8 3.5866854190826416\n","52 9 1.9674936532974243\n","52 8 2.959594249725342\n","52 8 4.014895439147949\n","52 8 1.3091635704040527\n","52 8 1.415740966796875\n","53 8 1.7992398738861084\n","53 8 1.6730177402496338\n","53 8 1.7479641437530518\n","53 8 2.968148708343506\n","53 9 1.318638801574707\n","53 8 3.186922788619995\n","53 9 3.078519821166992\n","53 8 2.76188588142395\n","53 8 1.2138006687164307\n","53 8 1.5142415761947632\n","54 9 3.427079439163208\n","54 9 2.7788760662078857\n","54 8 3.438800811767578\n","54 5 3.353911876678467\n","54 9 1.6371053457260132\n","54 9 2.060148239135742\n","54 1 1.407713532447815\n","54 8 1.0883560180664062\n","54 8 1.8147547245025635\n","54 1 2.1217308044433594\n","55 1 1.4767353534698486\n","55 1 1.5179455280303955\n","55 1 1.8151742219924927\n","55 1 1.1460630893707275\n","55 8 2.193164348602295\n","55 1 1.938830852508545\n","55 1 3.361994981765747\n","55 8 0.9816012382507324\n","55 8 2.1730027198791504\n","55 1 1.7658016681671143\n","56 8 3.2877414226531982\n","56 9 2.3242712020874023\n","56 8 1.4282288551330566\n","56 8 1.379016637802124\n","56 8 2.7168354988098145\n","56 8 2.1977529525756836\n","56 8 2.817523956298828\n","56 8 2.6813902854919434\n","56 8 1.1939655542373657\n","56 7 1.7690706253051758\n","57 8 1.6232068538665771\n","57 9 1.4284069538116455\n","57 9 2.7121548652648926\n","57 9 1.641121506690979\n","57 9 3.073115825653076\n","57 9 2.1599831581115723\n","57 9 1.770412564277649\n","57 8 2.0982041358947754\n","57 9 1.7898485660552979\n","57 1 1.6629724502563477\n","58 8 3.397334575653076\n","58 9 5.236626148223877\n","58 1 2.3403725624084473\n","58 1 1.4344724416732788\n","58 8 3.551389217376709\n","58 8 1.706267237663269\n","58 8 1.4885414838790894\n","58 1 1.0085585117340088\n","58 1 1.5627864599227905\n","58 1 2.4815707206726074\n","59 1 3.0930304527282715\n","59 8 3.0177133083343506\n","59 1 1.4486020803451538\n","59 1 3.1940791606903076\n","59 1 2.6066842079162598\n","59 1 2.6320230960845947\n","59 1 3.7758307456970215\n","59 8 1.4283512830734253\n","59 8 2.4563896656036377\n","59 1 3.055371046066284\n","60 9 2.4267654418945312\n","60 5 1.8163429498672485\n","60 1 3.603621006011963\n","60 1 2.87917423248291\n","60 8 1.8279308080673218\n","60 1 1.8712923526763916\n","60 3 2.438439130783081\n","60 8 1.1603690385818481\n","60 8 2.6084916591644287\n","60 9 2.013542652130127\n","61 8 1.514366865158081\n","61 5 3.021250009536743\n","61 1 1.5431548357009888\n","61 1 2.5043153762817383\n","61 8 2.4603610038757324\n","61 1 2.766622304916382\n","61 8 1.9716640710830688\n","61 8 1.81805419921875\n","61 1 1.4554762840270996\n","61 8 1.6674513816833496\n","62 8 1.0175340175628662\n","62 8 1.7901984453201294\n","62 8 1.413131833076477\n","62 8 2.767157793045044\n","62 8 1.8617289066314697\n","62 1 2.666872501373291\n","62 1 2.0941975116729736\n","62 1 1.9466439485549927\n","62 8 1.341784954071045\n","62 8 1.5247008800506592\n","63 8 1.951188564300537\n","63 1 1.4239208698272705\n","63 8 1.3813616037368774\n","63 8 2.6989479064941406\n","63 8 3.7342708110809326\n","63 8 2.0732455253601074\n","63 1 2.7509398460388184\n","63 8 2.837161064147949\n","63 8 2.4188473224639893\n","63 8 1.974101185798645\n","64 8 2.0332322120666504\n","64 1 2.3918282985687256\n","64 1 1.33299720287323\n","64 1 2.3676578998565674\n","64 1 1.2667429447174072\n","64 1 2.246058464050293\n","64 1 2.6120355129241943\n","64 8 1.5916721820831299\n","64 1 2.92753529548645\n","64 1 3.3413455486297607\n","65 1 3.216979503631592\n","65 1 1.858872890472412\n","65 1 2.7871217727661133\n","65 1 1.4141007661819458\n","65 1 2.2195444107055664\n","65 1 1.2674647569656372\n","65 1 2.3875675201416016\n","65 1 2.637800931930542\n","65 1 2.0096654891967773\n","65 1 2.216635227203369\n","66 1 1.933289647102356\n","66 5 1.7058604955673218\n","66 1 0.9548616409301758\n","66 1 2.318155288696289\n","66 1 2.720355272293091\n","66 1 2.3600685596466064\n","66 1 2.7026941776275635\n","66 1 0.9573254585266113\n","66 1 1.4623744487762451\n","66 1 2.725691795349121\n","67 1 2.292933940887451\n","67 1 2.6584439277648926\n","67 1 2.3340229988098145\n","67 1 2.200348138809204\n","67 1 1.9130181074142456\n","67 1 2.1222000122070312\n","67 8 2.471454620361328\n","67 8 1.8261988162994385\n","67 8 2.5164146423339844\n","67 9 1.5326324701309204\n","68 8 2.2121968269348145\n","68 9 1.9853556156158447\n","68 1 2.338696002960205\n","68 1 2.6887753009796143\n","68 8 3.420135736465454\n","68 1 2.4580636024475098\n","68 1 2.104100227355957\n","68 8 2.6561994552612305\n","68 8 2.0018157958984375\n","68 9 2.4906623363494873\n","69 5 2.192913055419922\n","69 5 2.1605405807495117\n","69 1 2.2326464653015137\n","69 1 1.4679585695266724\n","69 1 2.322416305541992\n","69 1 2.4286956787109375\n","69 1 2.2304420471191406\n","69 8 2.464383602142334\n","69 1 1.7351511716842651\n","69 7 2.6872634887695312\n","70 1 3.653817892074585\n","70 1 1.6885946989059448\n","70 5 1.52230966091156\n","70 5 2.1585023403167725\n","70 5 2.2774159908294678\n","70 1 1.4287340641021729\n","70 1 2.711698532104492\n","70 1 1.5733599662780762\n","70 1 2.211991786956787\n","70 1 1.4537447690963745\n","71 1 1.58647620677948\n","71 1 1.405245304107666\n","71 1 2.898777723312378\n","71 1 2.083871841430664\n","71 3 1.7342582941055298\n","71 1 1.6069791316986084\n","71 1 2.608717441558838\n","71 1 2.1641225814819336\n","71 1 1.559157133102417\n","71 8 1.5440459251403809\n","72 1 1.432589054107666\n","72 1 2.3774020671844482\n","72 1 2.1612296104431152\n","72 1 2.175121307373047\n","72 1 2.7134225368499756\n","72 1 2.6731057167053223\n","72 1 1.2170343399047852\n","72 1 1.0098705291748047\n","72 1 1.9280084371566772\n","72 1 2.013451099395752\n","73 8 2.156735420227051\n","73 1 2.072718620300293\n","73 1 3.9667558670043945\n","73 1 1.669898271560669\n","73 1 1.914358377456665\n","73 1 3.4567484855651855\n","73 1 1.2703295946121216\n","73 1 2.3961997032165527\n","73 8 2.7621548175811768\n","73 1 1.7715694904327393\n","74 8 1.83842933177948\n","74 5 2.250108242034912\n","74 5 2.7208235263824463\n","74 1 2.8806285858154297\n","74 8 2.020771026611328\n","74 3 1.9193906784057617\n","74 1 2.368860960006714\n","74 3 2.067348003387451\n","74 8 1.8066720962524414\n","74 5 1.9121638536453247\n","75 5 2.0446701049804688\n","75 5 2.3257203102111816\n","75 5 1.3955614566802979\n","75 5 2.873835325241089\n","75 5 1.9352147579193115\n","75 8 1.6985949277877808\n","75 5 1.9634654521942139\n","75 8 1.819075345993042\n","75 8 2.781949281692505\n","75 1 1.5371390581130981\n","76 5 3.3749988079071045\n","76 5 3.5765774250030518\n","76 5 1.7930835485458374\n","76 5 1.327345609664917\n","76 5 1.8290010690689087\n","76 5 3.231651544570923\n","76 1 2.13468074798584\n","76 8 3.7276487350463867\n","76 8 1.1973156929016113\n","76 5 2.8149797916412354\n","77 8 1.3099616765975952\n","77 8 2.8444178104400635\n","77 8 1.989121913909912\n","77 1 2.12768816947937\n","77 8 2.4446749687194824\n","77 7 1.9151923656463623\n","77 8 2.1826722621917725\n","77 5 1.8667385578155518\n","77 8 2.341468095779419\n","77 5 2.040815591812134\n","78 8 1.6589326858520508\n","78 5 1.3902289867401123\n","78 1 4.912504196166992\n","78 1 1.4192699193954468\n","78 5 1.6168291568756104\n","78 5 4.683809280395508\n","78 5 3.1076266765594482\n","78 1 1.8697314262390137\n","78 8 1.3198972940444946\n","78 1 1.8257235288619995\n","79 8 2.865236759185791\n","79 5 1.7824828624725342\n","79 1 2.77042293548584\n","79 1 1.6672205924987793\n","79 8 2.6422929763793945\n","79 1 2.7101330757141113\n","79 1 1.3108495473861694\n","79 1 2.3752379417419434\n","79 8 1.2569079399108887\n","79 8 2.450017213821411\n","80 8 1.1995010375976562\n","80 8 2.572652816772461\n","80 8 1.972943663597107\n","80 8 3.240649938583374\n","80 1 2.250156879425049\n","80 8 2.091193914413452\n","80 1 1.6822105646133423\n","80 1 1.7614390850067139\n","80 1 1.5268990993499756\n","80 5 2.0490288734436035\n","81 5 1.7162708044052124\n","81 1 2.123649835586548\n","81 1 1.5927850008010864\n","81 1 1.4503755569458008\n","81 1 1.5627164840698242\n","81 1 1.986520528793335\n","81 1 2.277128219604492\n","81 8 2.7468326091766357\n","81 1 2.6716392040252686\n","81 7 4.0118088722229\n","82 7 2.9326894283294678\n","82 5 2.547539234161377\n","82 1 2.5778965950012207\n","82 1 1.5016038417816162\n","82 1 3.0968806743621826\n","82 1 2.671945571899414\n","82 1 2.026336669921875\n","82 1 1.459889531135559\n","82 8 3.5702850818634033\n","82 7 1.5776385068893433\n","83 8 2.4637036323547363\n","83 1 1.6273462772369385\n","83 1 1.558289647102356\n","83 1 2.5837652683258057\n","83 1 1.0722393989562988\n","83 1 2.629110336303711\n","83 1 2.45340633392334\n","83 1 2.171664237976074\n","83 1 1.1879938840866089\n","83 1 3.770512342453003\n","84 1 2.117176055908203\n","84 1 3.4192519187927246\n","84 1 1.5351682901382446\n","84 1 2.0696773529052734\n","84 1 1.365319013595581\n","84 1 2.026608943939209\n","84 1 2.0731201171875\n","84 1 1.2757657766342163\n","84 1 1.5133579969406128\n","84 1 1.1875998973846436\n","85 1 1.0861976146697998\n","85 1 2.3024239540100098\n","85 1 2.829218864440918\n","85 1 3.0196566581726074\n","85 1 1.153301477432251\n","85 1 3.4656879901885986\n","85 1 1.8537976741790771\n","85 1 2.3692402839660645\n","85 1 1.126396656036377\n","85 1 3.444784164428711\n","86 1 1.5688964128494263\n","86 1 2.6423659324645996\n","86 1 2.20277738571167\n","86 1 1.6910659074783325\n","86 1 3.054232358932495\n","86 1 1.3903602361679077\n","86 1 2.2278783321380615\n","86 1 2.382373571395874\n","86 8 1.4941262006759644\n","86 7 2.109128475189209\n","87 8 2.571587324142456\n","87 1 1.9118419885635376\n","87 1 2.4665720462799072\n","87 1 3.4884421825408936\n","87 1 2.023383140563965\n","87 1 3.4698708057403564\n","87 1 1.3585023880004883\n","87 8 3.526576042175293\n","87 1 2.0923404693603516\n","87 7 1.641858696937561\n","88 1 3.240955352783203\n","88 1 3.4475228786468506\n","88 1 2.209537982940674\n","88 7 3.255647897720337\n","88 1 2.618117094039917\n","88 1 1.79995858669281\n","88 1 2.056041717529297\n","88 7 1.455023169517517\n","88 7 1.7860239744186401\n","88 1 2.138277769088745\n","89 7 1.9053828716278076\n","89 7 1.93486750125885\n","89 1 2.9164798259735107\n","89 1 1.9050503969192505\n","89 7 2.236964225769043\n","89 7 1.7479491233825684\n","89 1 1.586448073387146\n","89 1 1.87759530544281\n","89 7 2.5496702194213867\n","89 7 2.174198627471924\n","90 8 1.4683620929718018\n","90 8 2.1623990535736084\n","90 9 2.7412750720977783\n","90 1 1.6692824363708496\n","90 8 1.7253135442733765\n","90 7 2.179280996322632\n","90 8 1.463869333267212\n","90 8 2.79624605178833\n","90 8 1.8861980438232422\n","90 9 1.880550503730774\n","91 8 1.4479074478149414\n","91 8 1.7813016176223755\n","91 1 1.5743873119354248\n","91 1 2.5400400161743164\n","91 8 3.302086591720581\n","91 7 1.754612684249878\n","91 1 3.2899954319000244\n","91 8 1.5251703262329102\n","91 1 1.406258225440979\n","91 8 2.9365172386169434\n","92 8 3.123565435409546\n","92 1 1.3136579990386963\n","92 8 2.3620030879974365\n","92 1 2.2032721042633057\n","92 8 1.918505072593689\n","92 1 1.583967685699463\n","92 1 2.5787761211395264\n","92 1 2.743248701095581\n","92 1 2.360285520553589\n","92 1 1.8894710540771484\n","93 8 2.605382204055786\n","93 8 2.398364543914795\n","93 8 1.6214990615844727\n","93 1 2.293248414993286\n","93 1 1.565645456314087\n","93 1 5.1030659675598145\n","93 1 2.597752571105957\n","93 1 1.3399429321289062\n","93 1 1.6608515977859497\n","93 8 1.6046969890594482\n","94 1 2.0886716842651367\n","94 1 1.9985496997833252\n","94 1 1.7188247442245483\n","94 1 2.653489351272583\n","94 9 2.5712530612945557\n","94 1 1.9915896654129028\n","94 1 1.3787829875946045\n","94 1 1.8601967096328735\n","94 9 3.4001169204711914\n","94 9 1.8343900442123413\n","95 1 1.553389549255371\n","95 1 2.777848482131958\n","95 1 2.2310867309570312\n","95 1 2.3121273517608643\n","95 1 2.2973721027374268\n","95 1 1.3052752017974854\n","95 1 1.402942180633545\n","95 1 2.0078282356262207\n","95 1 2.4065372943878174\n","95 1 1.566056489944458\n","96 1 3.316746711730957\n","96 1 1.0475363731384277\n","96 1 2.2707464694976807\n","96 1 2.816368341445923\n","96 1 2.4524126052856445\n","96 1 2.601602792739868\n","96 1 1.7166348695755005\n","96 1 1.0518913269042969\n","96 1 1.7808691263198853\n","96 8 1.7277508974075317\n","97 1 2.459188461303711\n","97 1 1.9953396320343018\n","97 1 2.278602123260498\n","97 1 0.9945094585418701\n","97 1 1.2863197326660156\n","97 1 2.5735299587249756\n","97 1 2.204738140106201\n","97 8 2.8711140155792236\n","97 1 2.105198860168457\n","97 1 2.021343231201172\n","98 1 1.3751320838928223\n","98 1 2.5051193237304688\n","98 1 3.3682267665863037\n","98 1 1.9999401569366455\n","98 1 3.3619143962860107\n","98 1 2.0690979957580566\n","98 1 3.545536994934082\n","98 8 3.4911632537841797\n","98 1 2.2168760299682617\n","98 1 2.0577778816223145\n","99 8 1.8933660984039307\n","99 9 1.8686442375183105\n","99 8 2.624742031097412\n","99 8 1.9016470909118652\n","99 9 2.0138473510742188\n","99 9 2.85752010345459\n","99 8 3.405099391937256\n","99 8 1.9273091554641724\n","99 8 1.3341326713562012\n","99 8 1.4073140621185303\n","100 8 2.013533592224121\n","100 1 1.4570211172103882\n","100 1 1.4879425764083862\n","100 1 3.1697306632995605\n","100 8 2.3926796913146973\n","100 8 1.7386300563812256\n","100 8 1.7946006059646606\n","100 8 2.6625146865844727\n","100 8 1.9719024896621704\n","100 9 1.2349598407745361\n","101 9 3.1772520542144775\n","101 9 1.6984864473342896\n","101 8 2.9039156436920166\n","101 9 1.4859259128570557\n","101 9 2.4244282245635986\n","101 9 1.7320152521133423\n","101 8 1.612208604812622\n","101 1 1.2985066175460815\n","101 8 2.787722587585449\n","101 1 1.900641918182373\n","102 8 2.084779739379883\n","102 1 5.2080817222595215\n","102 1 1.9067391157150269\n","102 9 1.7354588508605957\n","102 1 1.9325859546661377\n","102 9 3.02166748046875\n","102 1 1.879623293876648\n","102 9 3.079552412033081\n","102 1 2.570035457611084\n","102 8 1.231588363647461\n","103 9 1.5324246883392334\n","103 8 1.5293594598770142\n","103 8 2.4576549530029297\n","103 8 2.8969955444335938\n","103 9 1.742212176322937\n","103 1 3.213163137435913\n","103 1 2.8937876224517822\n","103 1 2.776716947555542\n","103 8 2.331033229827881\n","103 8 1.40798020362854\n","104 8 2.755399703979492\n","104 9 2.09580397605896\n","104 8 2.514399528503418\n","104 1 1.3472344875335693\n","104 1 1.6155104637145996\n","104 9 2.1532883644104004\n","104 1 3.0657827854156494\n","104 1 2.3341825008392334\n","104 8 2.0157172679901123\n","104 9 1.9491146802902222\n","105 8 2.4013543128967285\n","105 8 1.489872932434082\n","105 8 1.440652847290039\n","105 8 2.2759311199188232\n","105 8 2.8525893688201904\n","105 8 1.963059902191162\n","105 8 2.29392409324646\n","105 8 1.7986619472503662\n","105 8 1.6092166900634766\n","105 8 1.3987717628479004\n","106 8 2.1179492473602295\n","106 8 1.6066778898239136\n","106 1 2.9237475395202637\n","106 1 2.6268699169158936\n","106 8 2.742419958114624\n","106 4 1.7335937023162842\n","106 1 2.0671586990356445\n","106 1 1.2843236923217773\n","106 1 1.9821611642837524\n","106 1 1.6883891820907593\n","107 8 1.702807903289795\n","107 1 2.6080751419067383\n","107 3 2.382171392440796\n","107 1 1.7914308309555054\n","107 1 1.4751898050308228\n","107 1 1.1802743673324585\n","107 1 2.3551511764526367\n","107 1 0.8812255859375\n","107 1 2.8612983226776123\n","107 1 1.4029855728149414\n","108 1 1.9432005882263184\n","108 1 2.3349857330322266\n","108 1 3.212599039077759\n","108 1 2.859750986099243\n","108 1 1.1928343772888184\n","108 1 0.995769739151001\n","108 1 4.451096057891846\n","108 1 2.881018877029419\n","108 1 1.1063258647918701\n","108 1 2.1357316970825195\n","109 1 1.6233928203582764\n","109 1 1.2137155532836914\n","109 1 1.2401347160339355\n","109 1 2.7360317707061768\n","109 1 2.557189702987671\n","109 1 1.942315936088562\n","109 1 3.3800454139709473\n","109 1 2.0154342651367188\n","109 8 2.259939670562744\n","109 8 1.3583694696426392\n","110 8 1.4940770864486694\n","110 8 1.3083641529083252\n","110 8 4.322714805603027\n","110 8 2.9074225425720215\n","110 8 2.735930919647217\n","110 1 1.1603509187698364\n","110 1 1.317463755607605\n","110 1 1.93156898021698\n","110 1 4.702770233154297\n","110 1 0.9384429454803467\n","111 1 3.0468671321868896\n","111 1 2.089102268218994\n","111 1 1.7991660833358765\n","111 1 1.7086224555969238\n","111 1 1.4054982662200928\n","111 1 3.21933913230896\n","111 1 1.3923918008804321\n","111 1 0.8917086124420166\n","111 8 1.3889892101287842\n","111 1 3.8822898864746094\n","112 1 1.378957986831665\n","112 1 4.768610954284668\n","112 1 3.2904562950134277\n","112 1 2.0870673656463623\n","112 1 0.9569506645202637\n","112 1 1.9868619441986084\n","112 1 1.811331033706665\n","112 1 1.0919220447540283\n","112 8 2.715646266937256\n","112 1 2.7812867164611816\n","113 1 1.3649224042892456\n","113 1 3.313229560852051\n","113 1 2.66497540473938\n","113 1 2.6824512481689453\n","113 1 2.5962369441986084\n","113 1 2.618579864501953\n","113 1 2.666543960571289\n","113 1 1.5325064659118652\n","113 1 1.127622365951538\n","113 8 2.238806962966919\n","114 8 2.7135605812072754\n","114 1 1.4153393507003784\n","114 1 2.1150686740875244\n","114 1 2.2764103412628174\n","114 1 2.508763313293457\n","114 1 2.201986789703369\n","114 1 3.1130313873291016\n","114 1 3.050673246383667\n","114 8 1.7702274322509766\n","114 1 2.336048126220703\n","115 1 1.9910364151000977\n","115 5 3.0345938205718994\n","115 5 1.7322198152542114\n","115 1 1.2772111892700195\n","115 1 2.3482069969177246\n","115 1 2.5185985565185547\n","115 1 2.9826626777648926\n","115 1 2.802929639816284\n","115 5 1.6671192646026611\n","115 1 2.2412803173065186\n","116 8 1.7112025022506714\n","116 9 1.9618871212005615\n","116 9 1.7293651103973389\n","116 1 1.1832355260849\n","116 1 1.1393685340881348\n","116 1 2.0242414474487305\n","116 1 1.238275408744812\n","116 1 1.0213897228240967\n","116 1 2.3185184001922607\n","116 1 1.5801472663879395\n","117 1 1.9963464736938477\n","117 1 2.3005049228668213\n","117 1 1.7948487997055054\n","117 1 1.113147497177124\n","117 1 2.4957118034362793\n","117 1 2.437272787094116\n","117 1 2.500150442123413\n","117 1 3.184007406234741\n","117 1 1.929244041442871\n","117 1 2.133237361907959\n","118 9 2.933302640914917\n","118 1 3.619584798812866\n","118 1 1.5810086727142334\n","118 1 1.6894481182098389\n","118 9 1.6451821327209473\n","118 9 2.72179913520813\n","118 1 1.4536973237991333\n","118 1 2.6409006118774414\n","118 8 2.8420863151550293\n","118 9 2.4964146614074707\n","119 9 2.5778164863586426\n","119 1 2.384282350540161\n","119 1 2.073948860168457\n","119 1 2.2734031677246094\n","119 1 2.225706100463867\n","119 9 1.5751217603683472\n","119 1 2.8084139823913574\n","119 1 2.3453545570373535\n","119 9 2.292558193206787\n","119 9 2.3496527671813965\n","120 9 2.2057790756225586\n","120 9 2.0130128860473633\n","120 8 3.625319480895996\n","120 9 1.7825965881347656\n","120 9 1.9195773601531982\n","120 9 1.7683510780334473\n","120 3 1.777439832687378\n","120 1 2.163079261779785\n","120 9 1.7851221561431885\n","120 9 1.762527585029602\n","121 9 3.159578323364258\n","121 9 1.597822666168213\n","121 9 1.636724829673767\n","121 9 2.94622802734375\n","121 9 2.5076093673706055\n","121 9 1.2172424793243408\n","121 9 1.3478460311889648\n","121 9 1.239220380783081\n","121 9 2.0657670497894287\n","121 9 1.936691403388977\n","122 9 1.786052942276001\n","122 9 1.3326005935668945\n","122 9 2.2267119884490967\n","122 9 3.684372663497925\n","122 9 1.5783635377883911\n","122 9 2.253011465072632\n","122 1 1.5452399253845215\n","122 1 3.0786733627319336\n","122 1 1.5633267164230347\n","122 1 1.7257331609725952\n","123 8 1.8021860122680664\n","123 1 1.4940226078033447\n","123 1 1.9628655910491943\n","123 1 2.2158939838409424\n","123 1 1.289057970046997\n","123 1 2.0954301357269287\n","123 1 1.9331227540969849\n","123 1 2.3321218490600586\n","123 1 1.128392219543457\n","123 1 1.7167303562164307\n","124 1 1.4939438104629517\n","124 1 1.752392053604126\n","124 1 1.8976110219955444\n","124 1 2.104663372039795\n","124 1 1.121483564376831\n","124 1 2.7765750885009766\n","124 1 2.310580253601074\n","124 1 1.3087936639785767\n","124 1 0.9838674068450928\n","124 1 1.701429843902588\n","125 1 3.269381523132324\n","125 1 3.170039176940918\n","125 1 1.059079647064209\n","125 1 1.022695779800415\n","125 1 2.0505099296569824\n","125 1 3.4155147075653076\n","125 1 0.8899857997894287\n","125 1 2.1982979774475098\n","125 1 2.3483505249023438\n","125 1 2.732546806335449\n","126 1 2.595313787460327\n","126 1 1.0836310386657715\n","126 1 3.8089756965637207\n","126 1 2.4787373542785645\n","126 1 2.353611707687378\n","126 1 2.436471462249756\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-39897c948384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"PKPz_z3Z55YU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JIPb1Sk55YW","colab_type":"code","colab":{}},"source":["# evaluate model\n","model.eval()\n","\n","np.random.seed(424242)\n","predseq = []\n","actualseq = []\n","for i in range(20):\n","    select_idx = np.random.randint(0,len(dvalid),1)[0]    \n","    input_ids = dvalid[select_idx].unsqueeze(0)\n","\n","    \n","#     for j in range(max_position_embeddings):\n","    for j in range(10):\n","\n","        tempval = 0\n","        tempval += input_ids[0, j].item()\n","        if tempval != 0:\n","            input_ids[0, j] = vocab_size-1 # mask label is the highest vocab number, never present in data\n","            predicted_aa = torch.argmax(model(input_ids)[0][0,j]).item() # TODO mask padding 0's for attention\n","            input_ids[0,j] = tempval # set masked value back to original value\n","#             print(model(input_ids)[0][0,j])\n","            predseq.append(predicted_aa)\n","            actualseq.append(tempval)\n","            print(i, predicted_aa,tempval)\n","\n","n_correct = 0\n","n_wrong = 0\n","for i, aa in enumerate(predseq):\n","    if aa == actualseq[i]:\n","        n_correct += 1\n","    else:\n","        n_wrong += 1\n","print(n_correct/(n_correct+n_wrong))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DOrHuRH_55YY","colab_type":"code","colab":{}},"source":["print(model(dvalid[2].unsqueeze(0))[0].shape)\n","print(torch.argmax(model(input_ids)[0][0,j]).item())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z2RpZdto55Ya","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NRKlpDWC55Yb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"19Smkiji55Yd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BtmrJyUJ55Ye","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwRPG3cX55Yg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oThB3Osi55Yi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"laiAuExQ55Yn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r7xT6bIj55Yo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsWkzh0t55Yq","colab_type":"code","colab":{}},"source":["def evaluate(model, data):\n","    model.eval()\n","    acc_list = []\n","    for d in data:\n","        outputs = model(d.unsqueeze(0), masked_lm_labels=d.unsqueeze(0))\n","        loss, prediction_scores = outputs[:2]\n","        \n","        predicted_index = torch.argmax(prediction_scores, dim=2)\n","        n_correct = torch.sum(predicted_index==d).item()\n","#         n_possible = torch.sum(d!=0).item()\n","        n_possible = len(predicted_index[0])\n","        acc = n_correct/n_possible\n","        acc_list.append(acc)\n","        \n","    return(np.mean(acc_list))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0tWGotR55Yy","colab_type":"code","colab":{}},"source":["# train model\n","batch_size = 1\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.01)#, momentum=args.momentum)\n","# optimizer = optim.AdamW(model.parameters())\n","_max_select = len(dtrain)\n","\n","\n","# single step to non-zero weights\n","optimizer.zero_grad()\n","select_idx = np.random.randint(0, _max_select, batch_size)\n","outputs = model(dtrain[select_idx], masked_lm_labels=dtrain[select_idx])\n","loss, prediction_scores = outputs[:2]\n","loss.backward()\n","optimizer.step()\n","\n","# # evaluate before training\n","# acc_train = evaluate(model, dtrain)\n","# acc_valid = evaluate(model, dvalid)\n","# print(acc_train)\n","# print(acc_valid)\n","\n","\n","# train\n","model.train()\n","np.random.seed(2019)\n","for i in range(100):\n","    optimizer.zero_grad()\n","    select_idx = np.random.randint(0, _max_select, batch_size)\n","    input_ids = tokens_tensor[select_idx]\n","    outputs = model(input_ids, masked_lm_labels=input_ids)\n","    loss, prediction_scores = outputs[:2]\n","    print(i, loss.item())\n","    loss.backward()\n","    optimizer.step()\n","    \n","#     loss.backward(retain_graph=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EH1AQEIR55Y0","colab_type":"code","colab":{}},"source":["# after training\n","acc_train = evaluate(model, dtrain)\n","acc_valid = evaluate(model, dvalid)\n","print(acc_train)\n","print(acc_valid)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cFBbjL0U55Y2","colab_type":"code","colab":{}},"source":["model.eval()\n","outputs = model(dvalid[0].unsqueeze(0), masked_lm_labels=dvalid[0].unsqueeze(0))\n","loss, prediction_scores = outputs[:2]\n","\n","predicted_index = torch.argmax(prediction_scores, dim=2)\n","print(predicted_index)\n","print(dvalid[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-S5jD8b55Y3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MrUL2vPe55Y5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UM8u0ry555Y6","colab_type":"code","colab":{}},"source":["\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJJWFg6E55Y8","colab_type":"code","colab":{}},"source":["# debug\n","model.train()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)#, momentum=args.momentum)\n","for i in range(10):\n","    optimizer.zero_grad()\n","#     select_idx = 1\n","#     input_ids = tokens_tensor[select_idx].unsqueeze(0)\n","#     input_ids = tokens_tensor[0:3]\n","    input_ids = dtrain[0].unsqueeze(0)\n","\n","    outputs = model(input_ids, masked_lm_labels=input_ids)\n","    loss, prediction_scores = outputs[:2]\n","    print(i, loss.item())\n","    loss.backward()\n","    optimizer.step()\n","    \n","#     loss.backward(retain_graph=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRXZhMQl55Y-","colab_type":"code","colab":{}},"source":["model.eval()\n","select_idx = 5\n","input_ids = tokens_tensor[select_idx].unsqueeze(0)\n","outputs = model(input_ids, masked_lm_labels=input_ids)\n","loss, prediction_scores = outputs[:2]\n","\n","predicted_index = torch.argmax(prediction_scores, dim=2)\n","a = evaluate(model, tokens_tensor[select_idx].unsqueeze(0))\n","print(a)\n","# print(predicted_index.shape)\n","print(predicted_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPNcl53N55ZA","colab_type":"code","colab":{}},"source":["# evaluate on random data to ensure accuracy metric works\n","model.eval()\n","\n","np.random.seed(2019)\n","noise = np.random.randint(1, 10, 512).reshape(1,-1)\n","input_ids = torch.from_numpy(noise).to('cuda')\n","\n","outputs = model(input_ids, masked_lm_labels=input_ids)\n","loss, prediction_scores = outputs[:2]\n","\n","predicted_index = torch.argmax(prediction_scores, dim=2)\n","a = evaluate(model, tokens_tensor[select_idx].unsqueeze(0))\n","print(a)\n","# print(predicted_index.shape)\n","print(input_ids)\n","print(predicted_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Vnu2wrN55ZC","colab_type":"code","colab":{}},"source":["# fix forward mask\n","model.eval()\n","\n","np.random.seed(2019)\n","noise = np.random.randint(1, 10, 512).reshape(1,-1)\n","input_ids = torch.from_numpy(noise).to('cuda')\n","input_ids[0,0] = -inf\n","\n","attention_mask = torch.from_numpy(np.ones(512).reshape(1,-1)).float().to('cuda')\n","attention_mask[0,0] = 0\n","# print(attention_mask)\n","\n","token_type_ids = torch.from_numpy(np.zeros(512).reshape(1,-1)).long().to('cuda')\n","\n","outputs = model(input_ids, masked_lm_labels=input_ids)\n","loss, prediction_scores = outputs[:2]\n","\n","predicted_index = torch.argmax(prediction_scores, dim=2)\n","a = evaluate(model, tokens_tensor[select_idx].unsqueeze(0))\n","print(a)\n","# print(predicted_index.shape)\n","print(input_ids)\n","print(predicted_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJau2_9b55ZE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XT2JkPMN55ZG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6j2KIu7f55ZI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7Z0_Jgm55ZK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASXO5gOa55ZN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PSU4rrdO55ZP","colab_type":"code","colab":{}},"source":["del model\n","torch.cuda.empty_cache()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aI-t2Rs555ZU","colab_type":"code","colab":{}},"source":["# train model\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def train_epoch(epoch, args, model, device, data_loader, optimizer):\n","    model.train()  # set to training mode, disappointingly does not actually train the model \n","    pid = os.getpid()\n","    for batch_idx, (data, target) in enumerate(data_loader):\n","        optimizer.zero_grad()\n","        output = model(data.to(device))\n","        loss = F.nll_loss(output, target.to(device))\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % args.log_interval == 0:\n","            print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n","                100. * batch_idx / len(data_loader), loss.item()))\n","            \n","optimizer = optim.SGD(model.parameters(), lr=0.01)#, momentum=args.momentum)\n","optimizer.zero_grad()\n","# output = model(tokens_tensor)\n","# loss = F.nll_loss(output, target.to('cuda'))\n","print([x for x in model.parameters()])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PoBBBQeO55ZV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iiY2TVZI55ZW","colab_type":"code","colab":{}},"source":["from transformers import BertTokenizer, BertModel, BertForMaskedLM\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","# model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model = BertForMaskedLM(config)\n","\n","input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n","outputs = model(input_ids, masked_lm_labels=input_ids)\n","loss, prediction_scores = outputs[:2]\n","print(loss)\n","print(prediction_scores)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxSiqrMs55ZY","colab_type":"code","colab":{}},"source":["print(input_ids.shape)\n","print(tokens_tensor[0].unsqueeze(0).shape)\n","print(tokens_tensor[0:2].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYID7shz55ZZ","colab_type":"code","colab":{}},"source":["from transformers import BertTokenizer, BertModel, BertForMaskedLM\n","\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","# model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model = BertForMaskedLM(config)\n","model.to('cuda')\n","\n","# input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n","# input_ids = tokens_tensor[0].unsqueeze(0)\n","input_ids = tokens_tensor[0:2]\n","\n","outputs = model(input_ids, masked_lm_labels=input_ids)\n","loss, prediction_scores = outputs[:2]\n","print(loss)\n","print(prediction_scores.shape)\n","\n","for i in range(2):\n","    input_ids = tokens_tensor[0:2]\n","    outputs = model(input_ids, masked_lm_labels=input_ids)\n","    loss, prediction_scores = outputs[:2]\n","    loss.backward()\n","    print(loss)\n","    print(prediction_scores.shape)\n","#     loss.backward(retain_graph=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfFqO-6r55Zd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tGFpnqCO55Zh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EXV_Oiij55Zi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qpPPk-sf55Zl","colab_type":"code","colab":{}},"source":["import torch\n","from transformers import BertTokenizer, BertModel, BertForMaskedLM\n","\n","# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n","# import logging\n","# logging.basicConfig(level=logging.INFO)\n","# logging.basicConfig(level=logging.NONE)\n","\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize input\n","text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n","tokenized_text = tokenizer.tokenize(text)\n","\n","# Mask a token that we will try to predict back with `BertForMaskedLM`\n","# masked_index = 8\n","# tokenized_text[masked_index] = '[MASK]'\n","\n","# Convert token to vocabulary indices\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","print(indexed_tokens)\n","# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n","segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n","\n","# Convert inputs to PyTorch tensors\n","tokens_tensor = torch.tensor([indexed_tokens])\n","segments_tensors = torch.tensor([segments_ids])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"evUYO0la55Zn","colab_type":"code","colab":{}},"source":["# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model.eval()\n","\n","# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","# segments_tensors = segments_tensors.to('cuda')\n","model.to('cuda')\n","\n","# Predict all tokens\n","with torch.no_grad():\n","#     outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    outputs = model(tokens_tensor)\n","\n","    predictions = outputs[0]\n","\n","# confirm we were able to predict 'henson'\n","predicted_index = torch.argmax(predictions[0, masked_index]).item()\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","print(predicted_token)\n","\n","predicted_index = torch.argmax(predictions[0, 11]).item()\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","print(predicted_token)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PX9CA45w55Zq","colab_type":"code","colab":{}},"source":["for i in range(len(indexed_tokens)):\n","    predicted_index = torch.argmax(predictions[0, i]).item()\n","    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","    print(predicted_token)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MQ04k5P255Zr","colab_type":"code","colab":{}},"source":["predicted_token = tokenizer.convert_ids_to_tokens([103])[0]\n","print(predicted_token)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tThRe7ba55Zt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gz4USnvY55Zv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wz46wnbx55Zw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KrqNGCF_55Zx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUsU5Ge655Zz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDVX4TcF55Z0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5L_l4eMA55Z1","colab_type":"code","colab":{}},"source":["import logging\n","logging.basicConfig(level=logging.INFO)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mH73ih9855Z2","colab_type":"code","colab":{}},"source":["# example tokenization\n","from transformers import BertTokenizer, BertModel, BertForMaskedLM\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n","tokenized_text = tokenizer.tokenize(text)\n","print(tokenized_text)\n","\n","masked_index = 8\n","tokenized_text[masked_index] = '[MASK]'\n","print(tokenized_text)\n","\n","indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n","tokens_tensor = torch.tensor([indexed_tokens])\n","print(tokens_tensor)\n","print(tokens_tensor.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YITbbsNm55Z3","colab_type":"code","colab":{}},"source":["# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n","model.eval()\n","\n","# If you have a GPU, put everything on cuda\n","tokens_tensor = tokens_tensor.to('cuda')\n","# segments_tensors = segments_tensors.to('cuda')\n","model.to('cuda')\n","\n","# Predict all tokens\n","with torch.no_grad():\n","#     outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n","    outputs = model(tokens_tensor)\n","    predictions = outputs[0]\n","\n","# confirm we were able to predict 'henson'\n","predicted_index = torch.argmax(predictions[0, masked_index]).item()\n","predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","print(predicted_token)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fd21GWBz55Z6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ezPuox855Z8","colab_type":"code","outputId":"fab38698-1d71-45cb-ec06-f869068dd13b","executionInfo":{"status":"ok","timestamp":1571673957351,"user_tz":240,"elapsed":283731,"user":{"displayName":"Markus Sommer","photoUrl":"","userId":"08083804319459268544"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# GPT-2\n","\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n","\n","# Load pre-trained model (weights)\n","model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n","\n","\n","# Set the model in evaluation mode to deactivate the DropOut modules\n","# This is IMPORTANT to have reproducible results during evaluation!\n","model.eval()\n","model.to('cuda')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["100%|██████████| 1042301/1042301 [00:01<00:00, 937069.71B/s]\n","100%|██████████| 456318/456318 [00:00<00:00, 500226.97B/s]\n","100%|██████████| 529/529 [00:00<00:00, 227614.57B/s]\n","100%|██████████| 3247202234/3247202234 [03:41<00:00, 14645396.05B/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 1280)\n","    (wpe): Embedding(1024, 1280)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (12): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (13): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (14): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (15): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (16): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (17): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (18): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (19): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (20): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (21): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (22): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (23): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (24): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (25): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (26): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (27): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (28): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (29): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (30): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (31): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (32): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (33): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (34): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (35): Block(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",")"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"Fj4V8Z8255Z_","colab_type":"code","colab":{}},"source":["# original_text = \"Martin Steinegger is in Peru because he \"\n","# original_text = \"The real reason Steven keeps recruiting German postdocs is \"\n","# original_text = \"The secret to giving a fun and compelling Joint Lab Meeting presentation is \"\n","# original_text = \"UC Berkeley is\"\n","# original_text = \"Why did Donald Trump \"\n","# original_text = \"Finding genes is easy... The secret is \"\n","# original_text = \"Computational gene finding is easy, the problem is \"\n","# original_text = \"The future of Biomedical Engineering is \"\n","# original_text = \"The best way to describe how neural networks work is \"\n","original_text = \"Improving on state-of-the-art bacterial gene finding programs is hard, \"\n","\n","\n","text = original_text\n","for i in range(20): # not the best way to iterate, but it works\n","    if text[-1] != \".\":\n","        # Encode a text inputs\n","        indexed_tokens = tokenizer.encode(text)\n","\n","        # Convert indexed tokens in a PyTorch tensor\n","        tokens_tensor = torch.tensor([indexed_tokens])\n","\n","        # If you have a GPU, put everything on cuda\n","        tokens_tensor = tokens_tensor.to('cuda')\n","\n","        # Predict all tokens\n","        with torch.no_grad():\n","            outputs = model(tokens_tensor)\n","            predictions = outputs[0]\n","\n","        # get the predicted next sub-word (in our case, the word 'man')\n","        predicted_index = torch.argmax(predictions[0, -1, :]).item()\n","        predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n","\n","        text = predicted_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GnlswbS155aA","colab_type":"code","outputId":"7ed70e38-64f6-4c01-861d-013d549c29bf","colab":{}},"source":["print(\"Original text:\\t\\t\", original_text)\n","print(\"Completed sentence:\\t\", predicted_text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Original text:\t\t Improving on state-of-the-art bacterial gene finding programs is hard, \n","Completed sentence:\t Improving on state-of-the-art bacterial gene finding programs is hard, but it's not impossible.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AT2RmpXZ55aC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFR-o-dr55aD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KKSSFryL55aE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aqxg6dGV55aF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUvkNFcZ55aH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vn9QworP55aI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7br8_zL55aL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0t_F1Fno55aL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ru6wfCmi55aN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}